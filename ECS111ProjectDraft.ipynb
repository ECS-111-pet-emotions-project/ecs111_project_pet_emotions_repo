{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68883d6a-6420-480f-883c-0b33ad7e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73446289-b0ab-40fd-bc8b-6a95112f9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.graph_objects as go\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102e2ed-b6e7-4bfa-bba6-8c347798f1fc",
   "metadata": {},
   "source": [
    "Define Data Path and Data Set Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89964b0-dbe6-4663-8b34-d7c105f980ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE NAMES\n",
    "train_data_dir = '/kaggle/input/pets-facial-expression-dataset/Master Folder/train'\n",
    "valid_data_dir = '/kaggle/input/pets-facial-expression-dataset/Master Folder/valid'\n",
    "test_data_dir = '/kaggle/input/pets-facial-expression-dataset/Master Folder/test'\n",
    "\n",
    "data_dir = '/kaggle/input/pets-facial-expression-dataset'\n",
    "\n",
    "\n",
    "ds_name = 'Pets Facial Expression'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e6726-9cdc-4d5a-aeac-37260f77a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "\n",
    "def generate_data_paths(data_dir):\n",
    "    \n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    folds = os.listdir(data_dir)\n",
    "    for fold in folds:\n",
    "        if fold == 'Master Folder':\n",
    "            continue\n",
    "            \n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "            filepaths.append(fpath)\n",
    "            labels.append(fold)\n",
    "            \n",
    "    return filepaths, labels\n",
    "\n",
    "\n",
    "filepaths, labels = generate_data_paths(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be56f7-36be-4f7c-8183-51ee77fe3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(filepaths, labels):\n",
    "\n",
    "    Fseries = pd.Series(filepaths, name= 'filepaths')\n",
    "    Lseries = pd.Series(labels, name='labels')\n",
    "    df = pd.concat([Fseries, Lseries], axis= 1)\n",
    "    return df\n",
    "\n",
    "df = create_df(filepaths, labels)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aad53-d24e-4ba7-af7a-aded47fe0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['labels'].value_counts())\n",
    "counts = df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f693e4b-0918-4834-a38b-cdca872aa5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df, name='df'):\n",
    "    num_images = df.shape[0]\n",
    "    num_classes = len(df['labels'].unique())\n",
    "    \n",
    "    print(f\"The {name} dataset has {num_images} images.\")\n",
    "    print(f\"The {name} dataset has {num_classes} classes.\")\n",
    "    \n",
    "    print(f\"The {name} dataset has:\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    for label, group in df.groupby('labels'):\n",
    "        num_class = len(group)\n",
    "        print(f\"Class '{label}' has {num_class} images\")\n",
    "        print('-' * 70)\n",
    "\n",
    "# Example Usage\n",
    "# Assuming df and ds_name are defined\n",
    "analyze_dataset(df, ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056b0bb-8792-454b-be7c-bd3669f7a51f",
   "metadata": {},
   "source": [
    "Define Data Path and Data Set Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb337f3-c3bc-476c-a408-441294a27182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values(df, name='df'):\n",
    "    \n",
    "    num_null_vals = sum(df.isnull().sum().values)\n",
    "    \n",
    "    if not num_null_vals:\n",
    "        print(f\"The {name} dataset has no null values\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"The {name} dataset has {num_null_vals} null values\")\n",
    "        print('-'*70)\n",
    "        print('Total null values in each column:\\n')\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "\n",
    "check_null_values(df, ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae282a4-476d-48ef-91f4-380fe26ad4aa",
   "metadata": {},
   "source": [
    "Spliting the dataframe into train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b21a2-08a5-4f60-8e25-10dd665fa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataframe\n",
    "train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123)\n",
    "\n",
    "# valid and test dataframe\n",
    "valid_df, test_df = train_test_split(dummy_df,  train_size= 0.6, shuffle= True, random_state= 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436a915-9871-44f6-9310-3b79953cc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset(train_df, \"Training \"+ds_name)\n",
    "analyze_dataset(valid_df, \"Validation \"+ds_name)\n",
    "analyze_dataset(test_df, \"Testing \"+ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef3cde-2606-47bf-b06c-4d12094918da",
   "metadata": {},
   "source": [
    "Create Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332baad-3cc9-4778-a897-04bcbd044e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crobed image size\n",
    "batch_size = 16\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "# Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "ts_length = len(test_df)\n",
    "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "def scalar(img):\n",
    "    return img\n",
    "\n",
    "tr_gen = ImageDataGenerator(preprocessing_function= scalar,\n",
    "                           rotation_range=40,\n",
    "                           width_shift_range=0.2,\n",
    "                           height_shift_range=0.2,\n",
    "                           brightness_range=[0.4,0.6],\n",
    "                           zoom_range=0.3,\n",
    "                           horizontal_flip=True,\n",
    "                           vertical_flip=True)\n",
    "\n",
    "ts_gen = ImageDataGenerator(preprocessing_function= scalar,\n",
    "                           rotation_range=40,\n",
    "                           width_shift_range=0.2,\n",
    "                           height_shift_range=0.2,\n",
    "                           brightness_range=[0.4,0.6],\n",
    "                           zoom_range=0.3,\n",
    "                           horizontal_flip=True,\n",
    "                           vertical_flip=True)\n",
    "\n",
    "train_gen = tr_gen.flow_from_dataframe(train_df, \n",
    "                                       x_col= 'filepaths', \n",
    "                                       y_col= 'labels', \n",
    "                                       target_size= img_size, \n",
    "                                       class_mode= 'categorical',\n",
    "                                       color_mode= 'rgb', \n",
    "                                       shuffle= True, \n",
    "                                       batch_size= batch_size)\n",
    "\n",
    "valid_gen = ts_gen.flow_from_dataframe(valid_df, \n",
    "                                       x_col= 'filepaths', \n",
    "                                       y_col= 'labels', \n",
    "                                       target_size= img_size, \n",
    "                                       class_mode= 'categorical',\n",
    "                                       color_mode= 'rgb', \n",
    "                                       shuffle= True, \n",
    "                                       batch_size= batch_size)\n",
    "\n",
    "# Note: we will use custom test_batch_size, and make shuffle= false\n",
    "test_gen = ts_gen.flow_from_dataframe(test_df, \n",
    "                                      x_col= 'filepaths', \n",
    "                                      y_col= 'labels', \n",
    "                                      target_size= img_size, \n",
    "                                      class_mode= 'categorical',\n",
    "                                      color_mode= 'rgb', \n",
    "                                      shuffle= False, \n",
    "                                      batch_size= test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf74cd5-3b66-4360-937c-b13f025ae4f4",
   "metadata": {},
   "source": [
    "Visualizing the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd34c9e-e7ab-4c66-83eb-7aa6405b8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Bar(\n",
    "            x= counts.values,\n",
    "            y=counts.index,\n",
    "            orientation='h'))\n",
    "\n",
    "fig.update_layout(title='Data Distribution in Bars',font_size=15,title_x=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8012eff-d264-4afe-b63e-24f941543e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../working/train/\"\n",
    "grouped_data = df.groupby(\"labels\")\n",
    "\n",
    "num_images_per_category = 5\n",
    "\n",
    "fig, axes = plt.subplots(len(grouped_data), num_images_per_category, figsize=(20, 20))\n",
    "\n",
    "for i, (category, group) in enumerate(grouped_data):\n",
    "  \n",
    "    random_indices = random.sample(range(len(group)), num_images_per_category)\n",
    "\n",
    "    for j, index in enumerate(random_indices):\n",
    "        filename = group.iloc[index][\"filepaths\"]\n",
    "        label = group.iloc[index][\"labels\"]\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        axes[i, j].imshow(image)\n",
    "        axes[i, j].set_title(\"Labels: \" + label, fontsize = 30)\n",
    "        \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39606cb-381d-496c-b8f3-0a5f41f514c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = train_gen.class_indices      # defines dictionary {'class': index}\n",
    "classes = list(g_dict.keys())       # defines list of dictionary's kays (classes), classes names : string\n",
    "images, labels = next(train_gen)      # get a batch size samples from the generator\n",
    "\n",
    "plt.figure(figsize= (20, 20))\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    image = images[i] / 255       # scales data to range (0 - 255)\n",
    "    plt.imshow(image)\n",
    "    index = np.argmax(labels[i])  # get image index\n",
    "    class_name = classes[index]   # get class of image\n",
    "    plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f347897-ef29-4dab-8223-613598c94727",
   "metadata": {},
   "source": [
    "Creating the Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f88bc0-1d23-4c45-b7b9-d71a0e78b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
    "\n",
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB5(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
    "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
    "    Dropout(rate= 0.45, seed= 123),\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc8ab8-42ed-4cd0-bc72-cb8ec996200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a08f5-0a36-4251-b776-b76f619059de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "model.compile(optimizer = RMSprop(learning_rate=0.0001), \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c380bbb-c875-479f-9a95-6590b1325dc0",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d347ef1-f056-410b-98cb-f4b78e927c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16   # set batch size for training\n",
    "history = model.fit(x=train_gen,\n",
    "                    epochs= 100,\n",
    "                    verbose= 1,\n",
    "                    validation_data= valid_gen, \n",
    "                    validation_steps= None,\n",
    "                    shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a846b-d380-4ddd-9712-cb4ff4da9c9f",
   "metadata": {},
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12275d15-db80-46b2-b42a-1d2f072af98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define needed variables\n",
    "tr_acc = history.history['accuracy']\n",
    "tr_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "index_loss = np.argmin(val_loss)\n",
    "val_lowest = val_loss[index_loss]\n",
    "index_acc = np.argmax(val_acc)\n",
    "acc_highest = val_acc[index_acc]\n",
    "Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "\n",
    "# Plot training history\n",
    "\n",
    "plt.figure(figsize= (20, 8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72032a-66f6-43d3-97b2-465a5516c600",
   "metadata": {},
   "source": [
    "Model Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a9d19-ae10-4e64-bb34-c453d341e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d213ca5-ecac-4faf-8b3b-b666c5e26029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Getting the Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558f247-bc91-4722-a170-7bf0c057bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_gen)\n",
    "y_pred = np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18637606-43e9-4cc0-a474-ba091684e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fd5db-10d6-4ed5-a9a6-d68da2bf080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = test_gen.class_indices\n",
    "classes = list(g_dict.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "\n",
    "plt.figure(figsize= (10, 10))\n",
    "plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation= 45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981d2d2-caa9-47ac-93e6-3f9c74f51c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61bc01-d126-4441-ad41-c580b0dd9cef",
   "metadata": {},
   "source": [
    "Loading the Model and Predicting Inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fd370-e6c7-48e3-ba7a-65529696e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "def predict_and_display(image_path, model):\n",
    "    \n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    \n",
    "    class_indices = train_gen.class_indices\n",
    "    class_labels = list(class_indices.keys())\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    if predicted_class_label == 'Other':\n",
    "        plt.title(f\"The pet is normal\")\n",
    "    else:\n",
    "        plt.title(f\"The Pet is {predicted_class_label}\")\n",
    "    plt.show()\n",
    "    \n",
    "model.load_weights('/kaggle/working/Pets_Facial_Expression_model.h5')\n",
    "\n",
    "class_labels = ['Angry', 'Other', 'Sad', 'Happy']\n",
    "\n",
    "# Replace 'path_to_test_image' with the path to the image you want to test\n",
    "image_path_to_test = '/kaggle/input/pets-facial-expression-dataset/Angry/02.jpg'\n",
    "predict_and_display(image_path_to_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
